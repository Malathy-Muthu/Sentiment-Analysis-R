---
title: "NLP & Text Mining-Course completion requirement for BBDS (Big Bang Data Science)"
author: "Malathy Muthu"
date: "6/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Free memory Functions

```{r}
# Clear environment
rm(list = ls()) 

# Clear packages
#pacman::p_unload(rgl)

# Clear plots
#dev.off()  # But only if there IS a plot

# Clear console
cat("\014")  # ctrl+L
```


#######################################################################################################
### Importing the RestReviews_DF
#######################################################################################################


```{r}
RestReviews_Org = read.delim(file.choose(), quote = '', stringsAsFactors = FALSE) # If we have tsv file 

# RestReviews_Org <- readLines(file.choose())  # If we have text file
```

```{r}
str(RestReviews_Org)
```


#######################################################################################################
### Cleaning the texts
#######################################################################################################

# "Corpus" is a collection of text documents.

# VCorpus in tm refers to "Volatile" corpus which means that the corpus is stored in memory and would 
# be destroyed when the R object containing it is destroyed.

# Contrast this with PCorpus or Permanent Corpus which are stored outside the memory say in a db.

# In order to create a VCorpus using tm, we need to pass a "Source" object as a paramter to the VCorpus
# method. 
```{r}
#https://www.red-gate.com/simple-talk/sql/bi/text-mining-and-sentiment-analysis-with-r/
#https://rpubs.com/chelseyhill/669117

install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
install.packages("syuzhet") # for sentiment analysis
install.packages("ggplot2") # for plotting graphs
install.packages("textstem") # for lemmatization
install.packages("lexicon") # for sentiment lexicon

```


```{r}
#install.packages('tm')
#install.packages('SnowballC')

library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(textstem)
library(lexicon)

```




# Find source available using VCorpus
  
# getSources()

```{r}
RestReviews_Corp = VCorpus(VectorSource(RestReviews_Org$Review))  # Create a Corpus 


```


```{r}
RestReviews_Corp
```

```{r}
as.character(RestReviews_Corp[[77]])  # Check the first record
```
```{r}
 #Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
RestReviews_Corp <- tm_map(RestReviews_Corp, toSpace, "/")
RestReviews_Corp <- tm_map(RestReviews_Corp, toSpace, "@")
RestReviews_Corp <- tm_map(RestReviews_Corp, toSpace, "!")
RestReviews_Corp <- tm_map(RestReviews_Corp, toSpace, "-")
RestReviews_Corp <- tm_map(RestReviews_Corp, toSpace, "\\|")
```


# Convert the text to lower case
```{r}
RestReviews_Corp <- tm_map(RestReviews_Corp, content_transformer(tolower))

```

```{r}
as.character(RestReviews_Corp[[77]])  # Check the first record
```


# Remove numbers
```{r}
as.character(RestReviews_Corp[[841]])  # Check the first record

RestReviews_Corp <- tm_map(RestReviews_Corp, removeNumbers)

as.character(RestReviews_Corp[[841]])  # Check the first record
```



# Remove punctuations
```{r}
as.character(RestReviews_Corp[[1]])  # Check the first record

RestReviews_Corp <- tm_map(RestReviews_Corp, removePunctuation)

as.character(RestReviews_Corp[[1]])  # Check the first record
```



# Remove english common stopwords
```{r}
as.character(RestReviews_Corp[[1]])  # Check the first record

RestReviews_Corp <- tm_map(RestReviews_Corp, removeWords, stopwords("english")) # Install SnowballC  package

as.character(RestReviews_Corp[[1]])  # Check the first record
```



# Remove your own stop word specify your stopwords as a character vector
# RestReviews_Corp = tm_map(docs, removeWords, c("blabla1", "blabla2"))

# Text stemming
```{r}

#as.character(RestReviews_Corp[[11]])  # Check the first record

#For sentiment analysis use lemmatize string from textstem package instead of stem from tm. 
#RestReviews_Corp <- tm_map(RestReviews_Corp, stemDocument)

#as.character(RestReviews_Corp[[11]])  # Check the first record
```

```{r}

word=c("servic", "absolut" ,"acknowledg",  "anytim", "attitud")
lem_word=lemmatize_words(word, dictionary = lexicon::hash_lemmas)
lem_word
```


# Eliminate extra white spaces
```{r}
RestReviews_Corp <- tm_map(RestReviews_Corp, stripWhitespace)

as.character(RestReviews_Corp[[1]])  # Check the first record
```


```{r}
#For sentiment analysis use lemmatize instead of stemming from the textstem package.
RestReviews_Corp <- tm_map(RestReviews_Corp, lemmatize_strings)
RestReviews_Corp <- tm_map(RestReviews_Corp, PlainTextDocument)
as.character(RestReviews_Corp[[11]])
```

```{r}
as.character(RestReviews_Corp[[11]])
```




```{r}
wordcloud(RestReviews_Corp, 
          min.freq = 250,
          max.words = 100,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))

wordcloud(RestReviews_Corp, 
          min.freq = 10,
          max.words = 250,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))
```


#######################################################################################################
### Creating the Bag of Words model
#######################################################################################################

```{r}
dtm = DocumentTermMatrix(RestReviews_Corp)  # dtm is a martix , but to make sure we have the right matrix
                                      # We should use as.matrix() when we want to tranform it to DF
dtm = removeSparseTerms(dtm, 0.999) # Filter non frequent words

dtm
```



#######################################################################################################
### Creating a data frame
#######################################################################################################

```{r}
RestReviews_DF = as.data.frame(as.matrix(dtm)) # Transforming a matrix to Data 
names(RestReviews_DF)


```
```{r}
# Plot the most frequent words
TextDoc_dtm <- TermDocumentMatrix(RestReviews_Corp)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
```


```{r}
head(dtm_d, 5)
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
```
```{r}

#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```
```{r}
# Find associations 
findAssocs(TextDoc_dtm, terms = c("good","food","place"), corlimit = 0.25)		
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)


```
```{r}

# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales
syuzhet_vector <- get_sentiment(RestReviews_Corp, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)
```

```{r}

# bing
bing_vector <- get_sentiment(RestReviews_Corp, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector <- get_sentiment(RestReviews_Corp, method="afinn")
head(afinn_vector)
summary(afinn_vector)
```
```{r}

#compare the first row of each vector using sign function
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)
```
```{r}

# run nrc sentiment analysis to return data frame with each row classified as one of the following
# emotions, rather than a score: 
# anger, anticipation, disgust, fear, joy, sadness, surprise, trust 
# It also counts the number of positive and negative emotions found in each row
d<-get_nrc_sentiment(as.character(RestReviews_Corp[]))
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
d

#d1<-get_nrc_sentiment(as.character(RestReviews_DF))
#head(d1,10)

```
```{r}

#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td[1:3]))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")
```
```{r}

#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)
```

```{r}
RestReviews_DF$Liked = RestReviews_Org$Liked  # Add dependent variable

names(RestReviews_DF)
```



#######################################################################################################
### Encoding the target feature as factor
#######################################################################################################

```{r}
RestReviews_DF$Liked = factor(RestReviews_DF$Liked, levels = c(0, 1))
```


#######################################################################################################
### Splitting the RestReviews_DF into the Training set and Test set
#######################################################################################################

# install.packages('caTools')
```{r}
library(caTools)
set.seed(123)

split = sample.split(RestReviews_DF$Liked, SplitRatio = 0.8)
training_set = subset(RestReviews_DF, split == TRUE)
test_set = subset(RestReviews_DF, split == FALSE)
```




#######################################################################################################
### Fitting Random Forest Classification to the Training set
#######################################################################################################

```{r}

#install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],  # X is the training set without dependent variable
                          y = training_set$Liked,
                          ntree = 100)
```


#######################################################################################################
### Predicting the Test set results
#######################################################################################################

```{r}
y_pred = predict(classifier, newdata = test_set[-692])

y_pred
```


#######################################################################################################
### Making the Confusion Matrix
#######################################################################################################

```{r}
cm = table(test_set[, 692], y_pred) 

cm
```

   

#######################################################################################################
### Calculate the accuracy // the proportion of the correct answer we have for the test set
#######################################################################################################

```{r}
sum(diag(cm))/sum(cm) # sum of the diagonal 
```




### Feature Scaling

```{r}
training_set[-692] = scale(training_set[-692]) # to make a cool graph to plot the prediction region and
# prediction boundary
test_set[-692] = scale(test_set[-692])
```

### Fitting Naive Bayes to the Training set

```{r}
#naiveBayes()


# install.packages('e1071')

library(e1071)

NaiveBayesModel= naiveBayes(x = training_set[-692],y = training_set$Liked)
#NaiveBayesModel
```

### Predicting the Test set results

```{r}
NB_pred = predict(NaiveBayesModel, newdata = test_set[-692])
```


```{r}
NB_pred
```

### Making the Confusion Matrix
```{r}
test_set[, 692]
```


```{r}
NBcm = table(test_set[, 692], NB_pred)
NBcm
```

### 14 incorrect predictions
```{r}
sum(diag(NBcm))/sum(NBcm) # sum of the diagonal 
```


